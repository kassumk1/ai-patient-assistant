# AI Patient Assistant (Offline)

A lightweight, local healthcare assistant built with Mistral-7B and `llama-cpp`. This tool runs fully offline and allows patients to ask health-related questions and receive AI-generated feedback, without sending any data online. Built with real-world AI engineering skills, itâ€™s designed for privacy, performance, and accessibility.

TO RUN ASSISTANT: python main.py
---

## ğŸš€ Features

- âš¡ **Offline Language Model (Mistral-7B)** â€” Fast, private inference via `llama-cpp-python`
- ğŸ¤– **Symptom Checker** â€” Custom prompts for health-related Q&A
- ğŸ” **Privacy-first** â€” No API keys or internet connection required
- ğŸ§  **Prompt Tuning** â€” Tailored interactions to guide responses for symptom checking
- ğŸ› ï¸ **Modular Python Code** â€” Easy to modify, scale, and test

---

## Getting Started

### Prerequisites

- Python 3.8 or newer  
- Mistral-7B model file (download separately)  
- `llama-cpp-python` installed

### Installation
This project uses the Mistral-7B-Instruct model from GPT4All, which is already in GGUF format and works with llama-cpp-python.

Visit: https://gpt4all.io/index.html
Scroll to the Downloads section
Choose the Mistral-7B-Instruct GGUF model
After downloading, place the .gguf file into your project directory, or update the model path in main.py if it's located elsewhere.
âš ï¸ Make sure the model is in GGUF format â€” other formats (like .safetensors) will not work with llama-cpp-python.
ğŸ’¡ Note: You can also use GGUF-format Mistral models from Hugging Face or other sources, as long as they are compatible with llama-cpp


```bash
git clone https://github.com/kassumk1/ai-patient-assistant.git
cd ai-patient-assistant
pip install -r requirements.txt
```
TO RUN ASSISTANT: python main.py




## My Goal for Developing this Project is to Demonstrate my Understanding of Technical Skills for AI Engineering:

* **Prompt Engineering** â€” Carefully crafted prompts to elicit medically relevant responses
* **Local Model Deployment** â€” Integrated Mistral-7B locally using llama-cpp, no APIs needed
* **AI Tool Integration** â€” Hands-on with model backends, token handling, and Python inference loops
* **AI-enhanced Data Interaction** â€” Engineered structured Q\&A using user intent analysis
* **System Design** â€” Architected a modular terminal-based tool for healthcare interaction
* **Open Source Dev Practices** â€” Used GitHub for collaboration, versioning, and transparency

---

## ğŸ“ Project Structure

ai-patient-assistant/
â”œâ”€â”€ main.py            # Runs the assistant
â”œâ”€â”€ prompts.py         # Prompt logic and formatting
â”œâ”€â”€ requirements.txt   # Dependencies
â”œâ”€â”€ README.md          # You're here!


---

## ğŸ›  Future Plans

* Memory support for multi-turn conversations
* Web-based interface (Flask or Streamlit)
* Fine-tuning on open-source medical datasets
* Role-based prompt variation (e.g., nurse, specialist, chatbot)

---

## ğŸ“Œ Notes

* This project is for educational and demo purposes.
* Not intended to replace medical advice from professionals.

---

## ğŸ“« Connect with Me

**Kassum Khan**
Cognitive Science Major 
[LinkedIn](https://www.linkedin.com/in/kassum-khan-89484b310)
